{
 "metadata": {
  "name": "",
  "signature": "sha256:dd74ebb26f135803c11c3fe563ba9bd09a38d3bf160eaa97eeef5b827d3b39ba"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## This is a crash course demo intro to low level NLP - Tokenization, Stopwords, POS, Stemming, Concordances"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import re\n",
      "import itertools\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "download stopwords, part of speech taggers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = removeHeader(filelist[2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(raw)\n",
      "tokens[0:25]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import wordpunct_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordpunct_tokenize(raw)[0:25]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenize(raw)[0:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "StopWords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "english_stops = set(stopwords.words('english'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "english_stops"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "string.punctuation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted([token.lower() for token in tokens if token not in string.punctuation])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted([token.lower() for token in tokens if token not in string.punctuation and token not in english_stops])[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_tokens(tokens):\n",
      "    \"\"\" Lowercases, takes out punct and stopwords and short strings \"\"\"\n",
      "    tokens = [token.lower() for token in tokens]  # do this first - otherwise it won't be stripped in stopwords cleaning\n",
      "    return sorted([token for token in tokens if (token not in string.punctuation) and \n",
      "                   (token not in english_stops) and len(token) > 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean = clean_tokens(tokens)\n",
      "clean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import Text\n",
      "text = Text(clean)\n",
      "text.vocab().most_common()[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "# NLTK Text Collection methods"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeTextCollection(textlist):\n",
      "    from nltk import Text\n",
      "    from nltk import TextCollection\n",
      "    texts= [Text(nltk.word_tokenize(clean_message(text))) for text in textlist]\n",
      "    collection = TextCollection(texts)\n",
      "    return collection"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceCol = makeTextCollection(filelist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceCol.concordance('shuttle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceCol.similar('shuttle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceCol.collocations(num=25, window_size=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceCol.dispersion_plot(['NASA','shuttle','air', 'force'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceCol.plot(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceCol.vocab().most_common()[0:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceCol.common_contexts(['space','shuttle'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(spaceCol.vocab())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stemming / Lemmatizing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The goal is to merge data items that are the same, and reduce the number of features.  \"cats\" and \"cat\" might be the same thing, from a topic perspective."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stemming removes affixes.  This is the default choice for stemming although other algs exist\n",
      "from nltk.stem import PorterStemmer\n",
      "stemmer = PorterStemmer()\n",
      "stemmer.stem('believes')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lemmatizing transforms to root words using grammar rules. It is slower.\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "lemmatizer.lemmatize('cooking', pos='v')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer.lemmatize('cookbooks')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer.stem('cookbooks')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer.stem(lemmatizer.lemmatize('buses'))  # an apparently recommended compression recipe in Perkins Python 3 NLTK book"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Parts of Speech"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.word_tokenize(\"And now for something completely different\")\n",
      "tagged = nltk.pos_tag(text)  # there are a few options for taggers, details in NLTK books\n",
      "tagged"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.untag(tagged)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See the tags here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Usefulness of lemmatizing / stemming ... vocab reduction, indexing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize_and_stem(text):\n",
      "    tokens = [w.lower() for w in nltk.word_tokenize(clean_message(text))]\n",
      "    no_stops = [t for t in tokens if t not in english_stops and t not in string.punctuation and len(t)>2]\n",
      "    #return no_stops\n",
      "    stems = [lemmatizer.lemmatize(t) for t in no_stops]\n",
      "    return stems"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeStemTextCollection(textlist):\n",
      "    from nltk import Text\n",
      "    from nltk import TextCollection\n",
      "    tokens = [tokenize_and_stem(text) for text in textlist]\n",
      "    texts= [Text(text) for text in tokens]\n",
      "    collection = TextCollection(texts)\n",
      "    return collection"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceStems = makeStemTextCollection(filelist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceStems.concordance('probe')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceStems.collocations(num=25, window_size=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reminder of above\n",
      "spaceCol.collocations(num=25, window_size=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceStems.vocab().most_common()[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceStems.plot(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(spaceStems.vocab())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(spaceCol.vocab())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(spaceStems)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(spaceCol)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceStems.common_contexts(['nasa'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spaceStems.similar('shuttle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Stemming can be useful for search engines... Example from NLTK original book"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "porter = nltk.PorterStemmer()\n",
      "grail = nltk.corpus.webtext.words('grail.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class IndexedText(object):\n",
      "\n",
      "    def __init__(self, stemmer, text):\n",
      "        self._text = text\n",
      "        self._stemmer = stemmer\n",
      "        self._index = nltk.Index((self._stem(word), i)\n",
      "                                 for (i, word) in enumerate(text))\n",
      "\n",
      "    def concordance(self, word, width=40):\n",
      "        key = self._stem(word)\n",
      "        wc = int(width/4)                # words of context\n",
      "        for i in self._index[key]:\n",
      "            lcontext = ' '.join(self._text[i-wc:i])\n",
      "            rcontext = ' '.join(self._text[i:i+wc])\n",
      "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
      "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
      "            print(ldisplay, rdisplay)\n",
      "\n",
      "    def _stem(self, word):\n",
      "        return self._stemmer.stem(word).lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = IndexedText(porter, grail)\n",
      "text.concordance('lie')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.concordance('say')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}